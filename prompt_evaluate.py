# -*- coding: utf-8 -*-
"""Prompt_evaluate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_E9-vjhJhLANgmf8czCKrMS02hu1YP6s
"""

from google.colab import drive
drive.mount('/content/drive')

cd Final Project Deliverables

pwd

import pandas as pd

file_path = "/content/drive/.shortcut-targets-by-id/1TfKakLLhqJ_X1AQGWWP9Z1a05BY4Z5Uh/Final Project Deliverables/Project_Prompt_to_Promt_Style_Transfer_Dataset_evlauate_dataset.tsv"
df = pd.read_csv(file_path, sep="\t")  # TSV uses tab separator
df.head()

!pip install pandas nltk textstat

import pandas as pd
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
import textstat

# FIX for NLTK bug
nltk.download('punkt')
nltk.download('punkt_tab')

def lexical_diversity(text):
    words = word_tokenize(str(text).lower())
    return len(set(words)) / len(words) if len(words) > 0 else 0

def hapax_legomena_ratio(text):
    words = word_tokenize(str(text).lower())
    freq = {}
    for w in words:
        freq[w] = freq.get(w, 0) + 1
    hapax = sum(1 for w in freq if freq[w] == 1)
    return hapax / len(words) if len(words) > 0 else 0

def sentence_complexity(text):
    text = str(text)
    sentences = sent_tokenize(text)
    words = word_tokenize(text)
    if len(sentences) == 0:
        return {"avg_words_sentence": 0, "complex_word_percentage": 0}
    complex_words = [w for w in words if textstat.syllable_count(w) >= 3]
    return {
        "avg_words_sentence": len(words) / len(sentences),
        "complex_word_percentage": len(complex_words) / len(words) if len(words) else 0
    }

def evaluate_text(text):
    text = str(text)
    return {
        # Readability
        "flesch_reading_ease": textstat.flesch_reading_ease(text),
        "flesch_kincaid_grade": textstat.flesch_kincaid_grade(text),
        "gunning_fog": textstat.gunning_fog(text),
        "smog_index": textstat.smog_index(text),
        "automated_readability_index": textstat.automated_readability_index(text),

        # Lexical diversity
        "type_token_ratio": lexical_diversity(text),
        "hapax_legomena_ratio": hapax_legomena_ratio(text),

        # Sentence complexity
        **sentence_complexity(text)
    }

model_columns = {
    "Output_Model_1_ChatGPT_51": "chatgpt",
    "Output_Model_2_Perplexity_Pro": "perplexity",
    "Output_Model_3_Grok": "grok"
}

results = []

for idx, row in df.iterrows():
    row_metrics = {"SNO": row["SNO"], "Style": row["Style"]}

    for col, model_name in model_columns.items():
        metrics = evaluate_text(row[col])

        # Add prefix model_metricname
        for metric_name, value in metrics.items():
            row_metrics[f"{model_name}_{metric_name}"] = value

    results.append(row_metrics)

metrics_df = pd.DataFrame(results)

# Merge metrics back to original dataset
full_df = pd.concat([df, metrics_df.drop(columns=["SNO", "Style"])], axis=1)

# Save
full_output_path = "/content/drive/.shortcut-targets-by-id/1TfKakLLhqJ_X1AQGWWP9Z1a05BY4Z5Uh/Final Project Deliverables/model_metrics_full.tsv"
full_df.to_csv(full_output_path, sep="\t", index=False)

print("Saved: model_metrics_full.tsv")

cd ..

summary_rows = []

for style in df["Style"].unique():
    style_df = metrics_df[metrics_df["Style"] == style]

    summary = {"Style": style}

    for model_name in ["chatgpt", "perplexity", "grok"]:
        model_cols = [c for c in style_df.columns if c.startswith(model_name)]
        model_summary = style_df[model_cols].mean()

        for metric_name, value in model_summary.items():
            summary[f"{model_name}_avg_{metric_name}"] = value

    summary_rows.append(summary)

summary_df = pd.DataFrame(summary_rows)

summary_path = "/content/drive/.shortcut-targets-by-id/1TfKakLLhqJ_X1AQGWWP9Z1a05BY4Z5Uh/Final Project Deliverables/style_model_summary.tsv"
summary_df.to_csv(summary_path, sep="\t", index=False)

print("Saved: style_model_summary.tsv")

!pip install pandas matplotlib numpy

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

summary_path = "/content/drive/.shortcut-targets-by-id/1TfKakLLhqJ_X1AQGWWP9Z1a05BY4Z5Uh/Final Project Deliverables/style_model_summary.tsv"
summary_df = pd.read_csv(summary_path, sep="\t")

summary_df

metrics = [
    "flesch_reading_ease",
    "flesch_kincaid_grade",
    "gunning_fog",
    "smog_index",
    "automated_readability_index",
    "type_token_ratio",
    "hapax_legomena_ratio",
    "avg_words_sentence",
    "complex_word_percentage"
]

def radar_chart_for_style(df, style_name, metrics):

    style_row = df[df["Style"] == style_name].iloc[0]

    models = {
        "ChatGPT": "chatgpt_avg_chatgpt_",
        "Perplexity": "perplexity_avg_perplexity_",
        "Grok": "grok_avg_grok_"
    }

    data = []

    # Extract metric values model-wise
    for model_label, prefix in models.items():
        model_vals = []
        for m in metrics:
            col_name = prefix + m
            model_vals.append(style_row[col_name])
        data.append(model_vals)

    # Radar setup
    num_vars = len(metrics)
    angles = np.linspace(0, 2*np.pi, num_vars, endpoint=False)

    fig, ax = plt.subplots(figsize=(7,7), subplot_kw=dict(polar=True))

    # Plot each model
    for i, (model_label, _) in enumerate(models.items()):
        values = data[i]
        values_loop = values + [values[0]]
        angle_loop = np.concatenate((angles, [angles[0]]))

        ax.plot(angle_loop, values_loop, label=model_label, linewidth=2)
        ax.fill(angle_loop, values_loop, alpha=0.1)

    ax.set_xticks(angles)
    ax.set_xticklabels(metrics, fontsize=10)
    ax.set_title(f"Style: {style_name}", fontsize=16)
    ax.legend(loc="upper right", bbox_to_anchor=(1.3, 1.1))

    plt.show()

summary_df.columns.tolist()

for style in summary_df["Style"].unique():
    radar_chart_for_style(summary_df, style, metrics)

import pandas as pd
import numpy as np

summary_path = "/content/drive/.shortcut-targets-by-id/1TfKakLLhqJ_X1AQGWWP9Z1a05BY4Z5Uh/Final Project Deliverables/style_model_summary.tsv"
summary_df = pd.read_csv(summary_path, sep="\t")

readability_metrics = [
    "flesch_reading_ease",
    "flesch_kincaid_grade",
    "gunning_fog",
    "smog_index",
    "automated_readability_index"
]

lexical_metrics = [
    "type_token_ratio",
    "hapax_legomena_ratio"
]

complexity_metrics = [
    "avg_words_sentence",
    "complex_word_percentage"
]

model_prefixes = {
    "ChatGPT": "chatgpt_avg_chatgpt_",
    "Perplexity": "perplexity_avg_perplexity_",
    "Grok": "grok_avg_grok_"
}

summary_rows = []

for style in summary_df["Style"].unique():
    row = summary_df[summary_df["Style"] == style].iloc[0]

    for model_name, prefix in model_prefixes.items():

        r_vals = [row[prefix + m] for m in readability_metrics]
        l_vals = [row[prefix + m] for m in lexical_metrics]
        c_vals = [row[prefix + m] for m in complexity_metrics]

        summary_rows.append({
            "Style": style,
            "Model": model_name,
            "Avg_Readability": np.mean(r_vals),
            "Avg_Lexical_Richness": np.mean(l_vals),
            "Avg_Complexity": np.mean(c_vals)
        })

summary_table = pd.DataFrame(summary_rows)
summary_table

def interpret_readability(score):
    if score > 60:
        return "very easy to read"
    elif score > 30:
        return "moderately readable"
    else:
        return "complex and difficult to read"

def interpret_lexical(score):
    if score > 0.5:
        return "high lexical diversity"
    elif score > 0.3:
        return "moderate diversity"
    else:
        return "simple vocabulary"

def interpret_complexity(score):
    if score > 20:
        return "long and complex sentences"
    elif score > 12:
        return "medium sentence complexity"
    else:
        return "short, simple sentences"


report_summaries = []

for _, row in summary_table.iterrows():
    r_phrase = interpret_readability(row["Avg_Readability"])
    l_phrase = interpret_lexical(row["Avg_Lexical_Richness"])
    c_phrase = interpret_complexity(row["Avg_Complexity"])

    paragraph = (
        f"For the **{row['Style']}** style, **{row['Model']}** produced responses that were "
        f"{r_phrase}, used {l_phrase}, and showed {c_phrase}. "
        f"This suggests that the model tends to adapt to the {row['Style']} style "
        f"with a characteristic writing pattern reflecting these linguistic traits."
    )

    report_summaries.append({
        "Style": row["Style"],
        "Model": row["Model"],
        "Summary": paragraph
    })

report_df = pd.DataFrame(report_summaries)
report_df

report_df.to_csv("/content/drive/.shortcut-targets-by-id/1TfKakLLhqJ_X1AQGWWP9Z1a05BY4Z5Uh/Final Project Deliverables/style_model_report_summary.tsv",
                 sep="\t", index=False)

print("Saved: style_model_report_summary.tsv")